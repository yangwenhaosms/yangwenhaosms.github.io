<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Wenhao Yang</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="CV.pdf">CV</a></div>
<div class="menu-item"><a href="pub.html" class="current">Publications</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="service.html">Service</a></div>
<div class="menu-category">Courses</div>
<div class="menu-item"><a href="322.html">MSE&nbsp;322</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>
<p>* denotes equal contribution or alphabetic ordering
<br /># denotes supervised student paper
</p>
<p><b><p><font size = 5> 2025 </font></p></b>
</p>
<ul>
<li><p>A Sequential Stopping Procedure for Statistical Estimation with Infinite Variance
<br />Jose Blanchet*, Peter Glynn*, <b>Wenhao Yang</b>*
<br />To be submitted
</p>
</li>
</ul>
<ul>
<li><p>Statistical Inference for the Stochastic Gradient Descent with Infinite Variance
<br />Jose Blanchet*, Peter Glynn*, <b>Wenhao Yang</b>*
<br />To be submitted
</p>
</li>
</ul>
<ul>
<li><p>Wasserstein Distributionally Robust Policy Learning with Continuous Context
<br /> Jose Blanchet*, Miao Lu*, <b>Wenhao Yang</b>*, Zhengyuan Zhou*
<br />To be submitted
</p>
</li>
</ul>
<p><b><p><font size = 5> 2024 </font></p></b>
</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.16340" target=&ldquo;blank&rdquo;>Limit Theorems for Stochastic Gradient Descent with Infinite Variance</a>
<br />Jose Blanchet*, Aleksandar Mijatović*, <b>Wenhao Yang</b>*
<br />The Annals of Applied Probability, Major Revision.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2408.00131" target=&ldquo;blank&rdquo;>Distributionally Robust Optimization as a Scalable Framework to Characterize Extreme Value Distributions</a>
<br />Patrick Kuiper, Ali Hasan, <b>Wenhao Yang</b>, Yuting Ng, Hoda Bidkhori, Jose Blanchet, Vahid Tarokh
<br />The 40th Conference on Uncertainty in Artificial Intelligence (UAI) 2024
</p>
</li>
</ul>
<p><b><p><font size = 5> 2023 </font></p></b>
</p>
<ul>
<li><p><a href="https://arxiv.org/pdf/2309.17262" target=&ldquo;blank&rdquo;>Estimation and Inference in Distributional Reinforcement Learning</a>
<br />Liangyu Zhang, Yang Peng, Jiadong Liang, <b>Wenhao Yang</b>#, Zhihua Zhang
<br />The Annals of Statistics, Accepted.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2305.00254.pdf" target=&ldquo;blank&rdquo;>Semi-Infinitely Constrained Markov Decision Processes and Efficient Reinforcement Learning</a>
<br />Liangyu Zhang, Yang Peng, <b>Wenhao Yang</b>, Zhihua Zhang
<br />Transactions on Pattern Analysis and Machine Intelligence
</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=6lP80vBiI6" target=&ldquo;blank&rdquo;>Semiparametrically Efficient Off-Policy Evaluation in Linear Markov Decision Processes</a>
<br />Chuhan Xie, <b>Wenhao Yang</b>#, Zhihua Zhang
<br />International Conference on Machine Learning (ICML) 2023
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2305.13185" target=&ldquo;blank&rdquo;>Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear MDPs: Theory and Practice</a>
<br />Toshinori Kitamura, Tadashi Kozuno, Yunhao Tang, Nino Vieillard, Michal Valko, <b>Wenhao Yang</b>, Jincheng Mei, Pierre MENARD, Mohammad Gheshlaghi Azar, Remi Munos, Olivier Pietquin, Matthieu Geist, Csaba Szepesvari, Wataru Kumagai, Yutaka Matsuo
<br />International Conference on Machine Learning (ICML) 2023
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2302.01248" target=&ldquo;blank&rdquo;>Robust Markov Decision Processes without Model Estimation</a>
<br /><b>Wenhao Yang</b>, Han Wang, Tadashi Kozuno, Scott M. Jordan, Zhihua Zhang
</p>
</li>
</ul>
<p><b><p><font size = 5> 2022 </font></p></b>
</p>
<ul>
<li><p><a href="https://openreview.net/pdf?id=ohk8bILFDkk" target=&ldquo;blank&rdquo;>Semi-Infinitely Constrained Markov Decision
Processes</a>
<br />Liangyu Zhang, Yang Peng,  <b>Wenhao Yang</b>, Zhihua Zhang
<br />Conference on Neural Information Processing Systems (NeurIPS) 2022.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2209.05186/" target=&ldquo;blank&rdquo;>Statistical Estimation of Confounded Linear MDPs: An Instrumental Variable Approach</a>
<br />Miao Lu*, <b>Wenhao Yang</b>*, Liangyu Zhang*, Zhihua Zhang*
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.14211/" target=&ldquo;blank&rdquo;>KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal</a>
<br />Tadashi Kozuno, <b>Wenhao Yang</b>, Nino Vieillard, Toshinori Kitamura, Yunhao Tang, Jincheng Mei, Pierre Ménard, Mohammad Gheshlaghi Azar, Michal Valko, Rémi Munos, Olivier Pietquin, Matthieu Geist, Csaba Szepesvári
</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.mlr.press/v151/jin22a.html" target=&ldquo;blank&rdquo;>Federated Reinforcement Learning with Environment Heterogeneity</a>
<br />Hao Jin, Yang Peng, <b>Wenhao Yang</b>, Shusen Wang, Zhihua Zhang
<br />International Conference on Artificial Intelligence and Statistics (AISTATS 2022).
</p>
</li>
</ul>
<p><b><p><font size = 5> 2021 </font></p></b>
</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.14582/" target=&ldquo;blank&rdquo;>Polyak-Ruppert-Averaged Q-Learning is Statistically Efficient</a>
<br />Xiang Li, <b>Wenhao Yang</b>, Jiadong Liang, Zhihua Zhang, Michael I. Jordan
<br />International Conference on Artificial Intelligence and Statistics (AISTATS 2023)
</p>
</li>
</ul>
<ul>
<li><p><a href="http://dx.doi.org/10.1214/22-AOS2225" target=&ldquo;blank&rdquo;>Towards Theoretical Understandings of Robust Markov Decision Processes: Sample Complexity and Asymptotics</a>
<br /><b>Wenhao Yang</b>, Liangyu Zhang, Zhihua Zhang
<br />The Annals of Statistics 2022, Vol. 50, No. 6, 3223-3248.
</p>
</li>
</ul>
<p><b><p><font size = 5> 2020 </font></p></b>
</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2011.00213/" target=&ldquo;blank&rdquo;>Finding the Near Optimal Policy via Adaptive Reduced Regularization in MDPs</a>
<br /><b>Wenhao Yang</b>, Xiang Li, Guangzeng Xie, Zhihua Zhang
<br />Workshop on Reinforcement Learning Theory at ICML 2021.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1907.02189/" target=&ldquo;blank&rdquo;>On the Convergence of FedAvg on Non-IID Data</a>
<br />Xiang Li*, Kaixuan Huang*, <b>Wenhao Yang</b>*, Shusen Wang, Zhihua Zhang
<br />International Conference on Learning Representations (ICLR) 2020. <b>(Oral)</b>
</p>
</li>
</ul>
<p><b><p><font size = 5> 2019 </font></p></b>
</p>
<ul>
<li><p><a href="https://arxiv.org/abs/1910.09126/" target=&ldquo;blank&rdquo;>Communication Efficient Decentralized Training with Multiple Local Updates</a> 
<br />Xiang Li, <b>Wenhao Yang</b>, Shusen Wang, Zhihua Zhang
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1903.00725/" target=&ldquo;blank&rdquo;>A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning</a>
<br />Xiang Li*, <b>Wenhao Yang</b>*, Zhihua Zhang
<br />Conference on Neural Information Processing Systems (NeurIPS) 2019.
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2025-09-26 15:00:15 PDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
