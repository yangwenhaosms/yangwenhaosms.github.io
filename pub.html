<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Wenhao Yang</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="CV_Wenhao_Yang.pdf">CV</a></div>
<div class="menu-item"><a href="pub.html" class="current">Publications</a></div>
<div class="menu-item"><a href="misc.html">Misc</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>
<p>* denotes equal contribution or alphabetic ordering</p>
<p><b><p><font size = 5> 2022 </font></p></b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2209.05186/" target=&ldquo;blank&rdquo;>Statistical Estimation of Confounded Linear MDPs: An Instrumental Variable Approach</a>
<br />Miao Lu*, <b>Wenhao Yang</b>*, Liangyu Zhang*, Zhihua Zhang*</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.14211/" target=&ldquo;blank&rdquo;>KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal</a>
<br />Tadashi Kozuno, <b>Wenhao Yang</b>, Nino Vieillard, Toshinori Kitamura, Yunhao Tang, Jincheng Mei, Pierre Ménard, Mohammad Gheshlaghi Azar, Michal Valko, Rémi Munos, Olivier Pietquin, Matthieu Geist, Csaba Szepesvári</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.mlr.press/v151/jin22a.html" target=&ldquo;blank&rdquo;>Federated Reinforcement Learning with Environment Heterogeneity</a>
<br />Hao Jin, Yang Peng, <b>Wenhao Yang</b>, Shusen Wang, Zhihua Zhang
<br />Proceedings of The 25th International Conference on Artificial Intelligence and Statistics (AISTATS 2022).</p>
</li>
</ul>
<p><b><p><font size = 5> 2021 </font></p></b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.14582/" target=&ldquo;blank&rdquo;>Polyak-Ruppert-Averaged Q-Learning is Statistically Efficient</a>
<br />Xiang Li, <b>Wenhao Yang</b>, Jiadong Liang, Zhihua Zhang, Michael I. Jordan</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2105.03863/" target=&ldquo;blank&rdquo;>Towards Theoretical Understandings of Robust Markov Decision Processes: Sample Complexity and Asymptotics</a>
<br /><b>Wenhao Yang</b>, Liangyu Zhang, Zhihua Zhang
<br />The Annals of Statistics (Accepted)</p>
</li>
</ul>
<p><b><p><font size = 5> 2020 </font></p></b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2011.00213/" target=&ldquo;blank&rdquo;>Finding the Near Optimal Policy via Adaptive Reduced Regularization in MDPs</a>
<br /><b>Wenhao Yang</b>, Xiang Li, Guangzeng Xie, Zhihua Zhang
<br />Workshop on Reinforcement Learning Theory at ICML 2021.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1907.02189/" target=&ldquo;blank&rdquo;>On the Convergence of FedAvg on Non-IID Data</a>
<br />Xiang Li*, Kaixuan Huang*, <b>Wenhao Yang</b>*, Shusen Wang, Zhihua Zhang
<br />International Conference on Learning Representations (ICLR) 2020. <b>(Oral)</b></p>
</li>
</ul>
<p><b><p><font size = 5> 2019 </font></p></b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/1910.09126/" target=&ldquo;blank&rdquo;>Communication Efficient Decentralized Training with Multiple Local Updates</a> 
<br />Xiang Li, <b>Wenhao Yang</b>, Shusen Wang, Zhihua Zhang</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1903.00725/" target=&ldquo;blank&rdquo;>A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning</a>
<br />Xiang Li*, <b>Wenhao Yang</b>*, Zhihua Zhang
<br />Conference on Neural Information Processing Systems (NeurIPS) 2019.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2022-10-17 14:05:13 MDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
